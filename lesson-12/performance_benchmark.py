#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìë™ë§¤ë§¤ ë°ì´í„° ë¶„ì„ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° í…ŒìŠ¤íŠ¸
ìµœì í™” ì „í›„ ì„±ëŠ¥ ë¹„êµ ë° ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
"""

import time
import psutil
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Tuple
from datetime import datetime, timedelta
import logging
import json
import gc
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from dataclasses import dataclass

# ì›ë³¸ ë° ìµœì í™”ëœ ëª¨ë“ˆ import
from data_processor import DataProcessor, DataConfig as OriginalDataConfig
from performance_metrics import calculate_comprehensive_metrics
from visualization import ChartGenerator
from statistical_analysis import StatisticalAnalyzer
from trading_analyzer import TradingAnalyzer, AnalysisConfig as OriginalAnalysisConfig

from data_processor_optimized import OptimizedDataProcessor, DataConfig as OptimizedDataConfig
from performance_metrics_optimized import OptimizedPerformanceAnalyzer
from visualization_optimized import OptimizedTradingVisualizer, ChartConfig
from statistical_analysis_optimized import OptimizedStatisticalAnalyzer
from trading_analyzer_optimized import OptimizedTradingAnalyzer, OptimizedAnalysisConfig

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class BenchmarkConfig:
    """ë²¤ì¹˜ë§ˆí¬ ì„¤ì •"""
    test_data_sizes: List[int] = None  # í…ŒìŠ¤íŠ¸í•  ë°ì´í„° í¬ê¸°ë“¤
    test_scenarios: List[str] = None   # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ë“¤
    iterations: int = 3                # ê° í…ŒìŠ¤íŠ¸ ë°˜ë³µ íšŸìˆ˜
    memory_limit_mb: int = 1000       # ë©”ëª¨ë¦¬ ì œí•œ
    timeout_seconds: int = 300         # íƒ€ì„ì•„ì›ƒ
    
    def __post_init__(self):
        if self.test_data_sizes is None:
            self.test_data_sizes = [1000, 5000, 10000, 50000, 100000]
        if self.test_scenarios is None:
            self.test_scenarios = ["small", "medium", "large", "xlarge"]

@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼"""
    test_name: str
    version: str  # "original" or "optimized"
    data_size: int
    execution_time: float
    memory_peak_mb: float
    memory_avg_mb: float
    cpu_usage_avg: float
    success: bool
    error_message: str = ""
    details: Dict[str, Any] = None

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.process = psutil.Process()
        self.monitoring = False
        self.memory_samples = []
        self.cpu_samples = []
        self.start_time = None
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.monitoring = True
        self.start_time = time.time()
        self.memory_samples = []
        self.cpu_samples = []
        
        # ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ
        import threading
        def monitor():
            while self.monitoring:
                try:
                    memory_mb = self.process.memory_info().rss / 1024 / 1024
                    cpu_percent = self.process.cpu_percent()
                    
                    self.memory_samples.append(memory_mb)
                    self.cpu_samples.append(cpu_percent)
                    
                    time.sleep(0.1)  # 100ms ê°„ê²©
                except:
                    break
        
        self.monitor_thread = threading.Thread(target=monitor, daemon=True)
        self.monitor_thread.start()
    
    def stop_monitoring(self) -> Tuple[float, float, float]:
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ ë° í†µê³„ ë°˜í™˜"""
        self.monitoring = False
        
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)
        
        execution_time = time.time() - self.start_time if self.start_time else 0
        
        peak_memory = max(self.memory_samples) if self.memory_samples else 0
        avg_memory = np.mean(self.memory_samples) if self.memory_samples else 0
        avg_cpu = np.mean(self.cpu_samples) if self.cpu_samples else 0
        
        return execution_time, peak_memory, avg_memory, avg_cpu

class DataGenerator:
    """í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±ê¸°"""
    
    @staticmethod
    def generate_test_data(size: int, scenario: str = "medium") -> Tuple[pd.DataFrame, pd.DataFrame]:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        np.random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼
        
        start_date = datetime.now() - timedelta(days=90)
        
        # ê±°ë˜ ë°ì´í„° ìƒì„±
        trades_data = []
        for i in range(size):
            trade_date = start_date + timedelta(
                days=np.random.randint(0, 90),
                hours=np.random.randint(0, 24),
                minutes=np.random.randint(0, 60)
            )
            
            trades_data.append({
                'order_id': f'order_{i:06d}',
                'symbol': np.random.choice(['KRW-BTC', 'KRW-ETH', 'KRW-XRP']),
                'side': np.random.choice(['BUY', 'SELL']),
                'amount': np.random.uniform(0.001, 0.01),
                'price': np.random.uniform(30000000, 70000000),
                'status': 'filled',
                'strategy': np.random.choice(['volatility_breakout', 'ma_crossover', 'rsi_strategy']),
                'created_at': trade_date,
                'updated_at': trade_date
            })
        
        trades_df = pd.DataFrame(trades_data)
        
        # ê³„ì¢Œ ë°ì´í„° ìƒì„±
        account_data = []
        balance = 10000000
        for i in range(90):  # 90ì¼ê°„ì˜ ê³„ì¢Œ ë°ì´í„°
            date = start_date + timedelta(days=i)
            daily_return = np.random.uniform(-0.05, 0.05)
            balance = balance * (1 + daily_return)
            
            account_data.append({
                'balance': balance,
                'date': date.date()
            })
        
        account_df = pd.DataFrame(account_data)
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ë°ì´í„° ì¡°ì •
        if scenario == "large":
            # ë” ë³µì¡í•œ ë°ì´í„°
            trades_df['pnl'] = np.random.uniform(-10000, 10000, len(trades_df))
            trades_df['holding_period'] = np.random.randint(1, 1440, len(trades_df))
        
        return trades_df, account_df

class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config: BenchmarkConfig = None):
        self.config = config or BenchmarkConfig()
        self.results = []
        self.logger = logging.getLogger(__name__)
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        self.results_dir = Path("benchmark_results")
        self.results_dir.mkdir(exist_ok=True)
    
    def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        self.logger.info("=== ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ===")
        
        benchmark_results = {
            'timestamp': datetime.now().isoformat(),
            'config': self.config.__dict__,
            'results': []
        }
        
        # 1. ë°ì´í„° ì²˜ë¦¬ ë²¤ì¹˜ë§ˆí¬
        self.logger.info("1. ë°ì´í„° ì²˜ë¦¬ ë²¤ì¹˜ë§ˆí¬")
        data_processing_results = self._benchmark_data_processing()
        benchmark_results['results'].extend(data_processing_results)
        
        # 2. ì„±ê³¼ ë¶„ì„ ë²¤ì¹˜ë§ˆí¬
        self.logger.info("2. ì„±ê³¼ ë¶„ì„ ë²¤ì¹˜ë§ˆí¬")
        performance_analysis_results = self._benchmark_performance_analysis()
        benchmark_results['results'].extend(performance_analysis_results)
        
        # 3. í†µê³„ ë¶„ì„ ë²¤ì¹˜ë§ˆí¬
        self.logger.info("3. í†µê³„ ë¶„ì„ ë²¤ì¹˜ë§ˆí¬")
        statistical_analysis_results = self._benchmark_statistical_analysis()
        benchmark_results['results'].extend(statistical_analysis_results)
        
        # 4. ì‹œê°í™” ë²¤ì¹˜ë§ˆí¬
        self.logger.info("4. ì‹œê°í™” ë²¤ì¹˜ë§ˆí¬")
        visualization_results = self._benchmark_visualization()
        benchmark_results['results'].extend(visualization_results)
        
        # 5. ì¢…í•© ë¶„ì„ ë²¤ì¹˜ë§ˆí¬
        self.logger.info("5. ì¢…í•© ë¶„ì„ ë²¤ì¹˜ë§ˆí¬")
        comprehensive_results = self._benchmark_comprehensive_analysis()
        benchmark_results['results'].extend(comprehensive_results)
        
        # ê²°ê³¼ ì €ì¥
        self._save_results(benchmark_results)
        
        # ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_performance_report(benchmark_results)
        
        self.logger.info("=== ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ ===")
        
        return benchmark_results
    
    def _benchmark_data_processing(self) -> List[BenchmarkResult]:
        """ë°ì´í„° ì²˜ë¦¬ ë²¤ì¹˜ë§ˆí¬"""
        results = []
        
        for size in self.config.test_data_sizes:
            self.logger.info(f"ë°ì´í„° ì²˜ë¦¬ ë²¤ì¹˜ë§ˆí¬: {size}ê±´")
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
            trades_df, account_df = DataGenerator.generate_test_data(size)
            
            # ì›ë³¸ ë²„ì „ í…ŒìŠ¤íŠ¸
            result = self._test_data_processing_original(trades_df, account_df, size)
            results.append(result)
            
            # ìµœì í™” ë²„ì „ í…ŒìŠ¤íŠ¸
            result = self._test_data_processing_optimized(trades_df, account_df, size)
            results.append(result)
        
        return results
    
    def _test_data_processing_original(self, trades_df: pd.DataFrame, account_df: pd.DataFrame, size: int) -> BenchmarkResult:
        """ì›ë³¸ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        monitor = PerformanceMonitor()
        
        try:
            monitor.start_monitoring()
            
            # ì›ë³¸ ë°ì´í„° ì²˜ë¦¬ê¸° ìƒì„± ë° í…ŒìŠ¤íŠ¸
            config = OriginalDataConfig()
            processor = DataProcessor(config)
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            processed_trades = processor.preprocess_trade_data(trades_df)
            processed_account = processor.preprocess_account_data(account_df)
            
            execution_time, peak_memory, avg_memory, avg_cpu = monitor.stop_monitoring()
            
            return BenchmarkResult(
                test_name="data_processing",
                version="original",
                data_size=size,
                execution_time=execution_time,
                memory_peak_mb=peak_memory,
                memory_avg_mb=avg_memory,
                cpu_usage_avg=avg_cpu,
                success=True,
                details={
                    'processed_trades': len(processed_trades),
                    'processed_account': len(processed_account)
                }
            )
            
        except Exception as e:
            monitor.stop_monitoring()
            return BenchmarkResult(
                test_name="data_processing",
                version="original",
                data_size=size,
                execution_time=0,
                memory_peak_mb=0,
                memory_avg_mb=0,
                cpu_usage_avg=0,
                success=False,
                error_message=str(e)
            )
    
    def _test_data_processing_optimized(self, trades_df: pd.DataFrame, account_df: pd.DataFrame, size: int) -> BenchmarkResult:
        """ìµœì í™”ëœ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        monitor = PerformanceMonitor()
        
        try:
            monitor.start_monitoring()
            
            # ìµœì í™”ëœ ë°ì´í„° ì²˜ë¦¬ê¸° ìƒì„± ë° í…ŒìŠ¤íŠ¸
            config = OptimizedDataConfig()
            processor = OptimizedDataProcessor(config)
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            processed_trades = processor.preprocess_trade_data_optimized(trades_df)
            processed_account = processor.preprocess_account_data_optimized(account_df)
            
            execution_time, peak_memory, avg_memory, avg_cpu = monitor.stop_monitoring()
            
            return BenchmarkResult(
                test_name="data_processing",
                version="optimized",
                data_size=size,
                execution_time=execution_time,
                memory_peak_mb=peak_memory,
                memory_avg_mb=avg_memory,
                cpu_usage_avg=avg_cpu,
                success=True,
                details={
                    'processed_trades': len(processed_trades),
                    'processed_account': len(processed_account)
                }
            )
            
        except Exception as e:
            monitor.stop_monitoring()
            return BenchmarkResult(
                test_name="data_processing",
                version="optimized",
                data_size=size,
                execution_time=0,
                memory_peak_mb=0,
                memory_avg_mb=0,
                cpu_usage_avg=0,
                success=False,
                error_message=str(e)
            )
    
    def _benchmark_performance_analysis(self) -> List[BenchmarkResult]:
        """ì„±ê³¼ ë¶„ì„ ë²¤ì¹˜ë§ˆí¬"""
        results = []
        
        for size in self.config.test_data_sizes:
            self.logger.info(f"ì„±ê³¼ ë¶„ì„ ë²¤ì¹˜ë§ˆí¬: {size}ê±´")
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
            trades_df, account_df = DataGenerator.generate_test_data(size)
            
            # ì›ë³¸ ë²„ì „ í…ŒìŠ¤íŠ¸
            result = self._test_performance_analysis_original(trades_df, account_df, size)
            results.append(result)
            
            # ìµœì í™” ë²„ì „ í…ŒìŠ¤íŠ¸
            result = self._test_performance_analysis_optimized(trades_df, account_df, size)
            results.append(result)
        
        return results
    
    def _test_performance_analysis_original(self, trades_df: pd.DataFrame, account_df: pd.DataFrame, size: int) -> BenchmarkResult:
        """ì›ë³¸ ì„±ê³¼ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        monitor = PerformanceMonitor()
        
        try:
            monitor.start_monitoring()
            
            # ì„±ê³¼ ì§€í‘œ ê³„ì‚°
            metrics = calculate_comprehensive_metrics(trades_df, account_df)
            
            execution_time, peak_memory, avg_memory, avg_cpu = monitor.stop_monitoring()
            
            return BenchmarkResult(
                test_name="performance_analysis",
                version="original",
                data_size=size,
                execution_time=execution_time,
                memory_peak_mb=peak_memory,
                memory_avg_mb=avg_memory,
                cpu_usage_avg=avg_cpu,
                success=True,
                details={'metrics_calculated': len(metrics.__dict__)}
            )
            
        except Exception as e:
            monitor.stop_monitoring()
            return BenchmarkResult(
                test_name="performance_analysis",
                version="original",
                data_size=size,
                execution_time=0,
                memory_peak_mb=0,
                memory_avg_mb=0,
                cpu_usage_avg=0,
                success=False,
                error_message=str(e)
            )
    
    def _test_performance_analysis_optimized(self, trades_df: pd.DataFrame, account_df: pd.DataFrame, size: int) -> BenchmarkResult:
        """ìµœì í™”ëœ ì„±ê³¼ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        monitor = PerformanceMonitor()
        
        try:
            monitor.start_monitoring()
            
            # ìµœì í™”ëœ ì„±ê³¼ ë¶„ì„ê¸°
            analyzer = OptimizedPerformanceAnalyzer()
            metrics = analyzer.calculate_comprehensive_metrics(trades_df, account_df)
            
            execution_time, peak_memory, avg_memory, avg_cpu = monitor.stop_monitoring()
            
            return BenchmarkResult(
                test_name="performance_analysis",
                version="optimized",
                data_size=size,
                execution_time=execution_time,
                memory_peak_mb=peak_memory,
                memory_avg_mb=avg_memory,
                cpu_usage_avg=avg_cpu,
                success=True,
                details={'metrics_calculated': len(metrics.__dict__)}
            )
            
        except Exception as e:
            monitor.stop_monitoring()
            return BenchmarkResult(
                test_name="performance_analysis",
                version="original",
                data_size=size,
                execution_time=0,
                memory_peak_mb=0,
                memory_avg_mb=0,
                cpu_usage_avg=0,
                success=False,
                error_message=str(e)
            )
    
    def _benchmark_statistical_analysis(self) -> List[BenchmarkResult]:
        """í†µê³„ ë¶„ì„ ë²¤ì¹˜ë§ˆí¬"""
        results = []
        
        for size in self.config.test_data_sizes:
            self.logger.info(f"í†µê³„ ë¶„ì„ ë²¤ì¹˜ë§ˆí¬: {size}ê±´")
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
            trades_df, account_df = DataGenerator.generate_test_data(size)
            
            # ì›ë³¸ ë²„ì „ í…ŒìŠ¤íŠ¸
            result = self._test_statistical_analysis_original(trades_df, account_df, size)
            results.append(result)
            
            # ìµœì í™” ë²„ì „ í…ŒìŠ¤íŠ¸
            result = self._test_statistical_analysis_optimized(trades_df, account_df, size)
            results.append(result)
        
        return results
    
    def _test_statistical_analysis_original(self, trades_df: pd.DataFrame, account_df: pd.DataFrame, size: int) -> BenchmarkResult:
        """ì›ë³¸ í†µê³„ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        monitor = PerformanceMonitor()
        
        try:
            monitor.start_monitoring()
            
            # ì›ë³¸ í†µê³„ ë¶„ì„ê¸°
            analyzer = StatisticalAnalyzer()
            results = analyzer.analyze_comprehensive_statistics(trades_df, account_df)
            
            execution_time, peak_memory, avg_memory, avg_cpu = monitor.stop_monitoring()
            
            return BenchmarkResult(
                test_name="statistical_analysis",
                version="original",
                data_size=size,
                execution_time=execution_time,
                memory_peak_mb=peak_memory,
                memory_avg_mb=avg_memory,
                cpu_usage_avg=avg_cpu,
                success=True,
                details={'statistical_tests': len(results)}
            )
            
        except Exception as e:
            monitor.stop_monitoring()
            return BenchmarkResult(
                test_name="statistical_analysis",
                version="original",
                data_size=size,
                execution_time=0,
                memory_peak_mb=0,
                memory_avg_mb=0,
                cpu_usage_avg=0,
                success=False,
                error_message=str(e)
            )
    
    def _test_statistical_analysis_optimized(self, trades_df: pd.DataFrame, account_df: pd.DataFrame, size: int) -> BenchmarkResult:
        """ìµœì í™”ëœ í†µê³„ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        monitor = PerformanceMonitor()
        
        try:
            monitor.start_monitoring()
            
            # ìµœì í™”ëœ í†µê³„ ë¶„ì„ê¸°
            analyzer = OptimizedStatisticalAnalyzer()
            results = analyzer.analyze_comprehensive_statistics(trades_df, account_df)
            
            execution_time, peak_memory, avg_memory, avg_cpu = monitor.stop_monitoring()
            
            return BenchmarkResult(
                test_name="statistical_analysis",
                version="optimized",
                data_size=size,
                execution_time=execution_time,
                memory_peak_mb=peak_memory,
                memory_avg_mb=avg_memory,
                cpu_usage_avg=avg_cpu,
                success=True,
                details={'statistical_tests': len(results)}
            )
            
        except Exception as e:
            monitor.stop_monitoring()
            return BenchmarkResult(
                test_name="statistical_analysis",
                version="optimized",
                data_size=size,
                execution_time=0,
                memory_peak_mb=0,
                memory_avg_mb=0,
                cpu_usage_avg=0,
                success=False,
                error_message=str(e)
            )
    
    def _benchmark_visualization(self) -> List[BenchmarkResult]:
        """ì‹œê°í™” ë²¤ì¹˜ë§ˆí¬"""
        results = []
        
        for size in self.config.test_data_sizes[:3]:  # ì‹œê°í™”ëŠ” ì‘ì€ ë°ì´í„°ë§Œ í…ŒìŠ¤íŠ¸
            self.logger.info(f"ì‹œê°í™” ë²¤ì¹˜ë§ˆí¬: {size}ê±´")
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
            trades_df, account_df = DataGenerator.generate_test_data(size)
            
            # ì›ë³¸ ë²„ì „ í…ŒìŠ¤íŠ¸
            result = self._test_visualization_original(trades_df, account_df, size)
            results.append(result)
            
            # ìµœì í™” ë²„ì „ í…ŒìŠ¤íŠ¸
            result = self._test_visualization_optimized(trades_df, account_df, size)
            results.append(result)
        
        return results
    
    def _test_visualization_original(self, trades_df: pd.DataFrame, account_df: pd.DataFrame, size: int) -> BenchmarkResult:
        """ì›ë³¸ ì‹œê°í™” í…ŒìŠ¤íŠ¸"""
        monitor = PerformanceMonitor()
        
        try:
            monitor.start_monitoring()
            
            # ì›ë³¸ ì°¨íŠ¸ ìƒì„±ê¸°
            chart_generator = ChartGenerator()
            
            # ê¸°ë³¸ ì°¨íŠ¸ ìƒì„±
            fig1 = chart_generator.create_equity_curve(account_df)
            fig2 = chart_generator.create_trade_distribution(trades_df)
            
            execution_time, peak_memory, avg_memory, avg_cpu = monitor.stop_monitoring()
            
            return BenchmarkResult(
                test_name="visualization",
                version="original",
                data_size=size,
                execution_time=execution_time,
                memory_peak_mb=peak_memory,
                memory_avg_mb=avg_memory,
                cpu_usage_avg=avg_cpu,
                success=True,
                details={'charts_created': 2}
            )
            
        except Exception as e:
            monitor.stop_monitoring()
            return BenchmarkResult(
                test_name="visualization",
                version="original",
                data_size=size,
                execution_time=0,
                memory_peak_mb=0,
                memory_avg_mb=0,
                cpu_usage_avg=0,
                success=False,
                error_message=str(e)
            )
    
    def _test_visualization_optimized(self, trades_df: pd.DataFrame, account_df: pd.DataFrame, size: int) -> BenchmarkResult:
        """ìµœì í™”ëœ ì‹œê°í™” í…ŒìŠ¤íŠ¸"""
        monitor = PerformanceMonitor()
        
        try:
            monitor.start_monitoring()
            
            # ìµœì í™”ëœ ì‹œê°í™” ìƒì„±ê¸°
            chart_config = ChartConfig()
            visualizer = OptimizedTradingVisualizer(chart_config)
            
            # ìµœì í™”ëœ ì°¨íŠ¸ ìƒì„±
            fig1 = visualizer.create_equity_curve_optimized(account_df)
            fig2 = visualizer.create_trade_distribution_optimized(trades_df)
            
            execution_time, peak_memory, avg_memory, avg_cpu = monitor.stop_monitoring()
            
            return BenchmarkResult(
                test_name="visualization",
                version="optimized",
                data_size=size,
                execution_time=execution_time,
                memory_peak_mb=peak_memory,
                memory_avg_mb=avg_memory,
                cpu_usage_avg=avg_cpu,
                success=True,
                details={'charts_created': 2}
            )
            
        except Exception as e:
            monitor.stop_monitoring()
            return BenchmarkResult(
                test_name="visualization",
                version="optimized",
                data_size=size,
                execution_time=0,
                memory_peak_mb=0,
                memory_avg_mb=0,
                cpu_usage_avg=0,
                success=False,
                error_message=str(e)
            )
    
    def _benchmark_comprehensive_analysis(self) -> List[BenchmarkResult]:
        """ì¢…í•© ë¶„ì„ ë²¤ì¹˜ë§ˆí¬"""
        results = []
        
        for size in self.config.test_data_sizes[:3]:  # ì¢…í•© ë¶„ì„ì€ ì‘ì€ ë°ì´í„°ë§Œ í…ŒìŠ¤íŠ¸
            self.logger.info(f"ì¢…í•© ë¶„ì„ ë²¤ì¹˜ë§ˆí¬: {size}ê±´")
            
            # ì›ë³¸ ë²„ì „ í…ŒìŠ¤íŠ¸
            result = self._test_comprehensive_analysis_original(size)
            results.append(result)
            
            # ìµœì í™” ë²„ì „ í…ŒìŠ¤íŠ¸
            result = self._test_comprehensive_analysis_optimized(size)
            results.append(result)
        
        return results
    
    def _test_comprehensive_analysis_original(self, size: int) -> BenchmarkResult:
        """ì›ë³¸ ì¢…í•© ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        monitor = PerformanceMonitor()
        
        try:
            monitor.start_monitoring()
            
            # ì›ë³¸ ë¶„ì„ê¸°
            config = OriginalAnalysisConfig()
            analyzer = TradingAnalyzer(config)
            
            # ì¢…í•© ë¶„ì„ ì‹¤í–‰
            results = analyzer.run_comprehensive_analysis()
            
            execution_time, peak_memory, avg_memory, avg_cpu = monitor.stop_monitoring()
            
            return BenchmarkResult(
                test_name="comprehensive_analysis",
                version="original",
                data_size=size,
                execution_time=execution_time,
                memory_peak_mb=peak_memory,
                memory_avg_mb=avg_memory,
                cpu_usage_avg=avg_cpu,
                success=True,
                details={'analysis_completed': bool(results)}
            )
            
        except Exception as e:
            monitor.stop_monitoring()
            return BenchmarkResult(
                test_name="comprehensive_analysis",
                version="original",
                data_size=size,
                execution_time=0,
                memory_peak_mb=0,
                memory_avg_mb=0,
                cpu_usage_avg=0,
                success=False,
                error_message=str(e)
            )
    
    def _test_comprehensive_analysis_optimized(self, size: int) -> BenchmarkResult:
        """ìµœì í™”ëœ ì¢…í•© ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        monitor = PerformanceMonitor()
        
        try:
            monitor.start_monitoring()
            
            # ìµœì í™”ëœ ë¶„ì„ê¸°
            config = OptimizedAnalysisConfig()
            analyzer = OptimizedTradingAnalyzer(config)
            
            # ì¢…í•© ë¶„ì„ ì‹¤í–‰
            results = analyzer.run_comprehensive_analysis()
            
            execution_time, peak_memory, avg_memory, avg_cpu = monitor.stop_monitoring()
            
            return BenchmarkResult(
                test_name="comprehensive_analysis",
                version="optimized",
                data_size=size,
                execution_time=execution_time,
                memory_peak_mb=peak_memory,
                memory_avg_mb=avg_memory,
                cpu_usage_avg=avg_cpu,
                success=True,
                details={'analysis_completed': bool(results)}
            )
            
        except Exception as e:
            monitor.stop_monitoring()
            return BenchmarkResult(
                test_name="comprehensive_analysis",
                version="optimized",
                data_size=size,
                execution_time=0,
                memory_peak_mb=0,
                memory_peak_mb=0,
                memory_avg_mb=0,
                cpu_usage_avg=0,
                success=False,
                error_message=str(e)
            )
    
    def _save_results(self, benchmark_results: Dict[str, Any]):
        """ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"benchmark_results_{timestamp}.json"
        filepath = self.results_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(benchmark_results, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥: {filepath}")
    
    def _generate_performance_report(self, benchmark_results: Dict[str, Any]):
        """ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""
        # ê²°ê³¼ ë¶„ì„
        df_results = pd.DataFrame([result.__dict__ for result in benchmark_results['results']])
        
        # ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ë§Œ í•„í„°ë§
        successful_results = df_results[df_results['success'] == True]
        
        if successful_results.empty:
            self.logger.warning("ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ê°€ ì—†ì–´ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = self._create_performance_report(successful_results)
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = self.results_dir / f"performance_report_{timestamp}.html"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        self.logger.info(f"ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±: {report_file}")
    
    def _create_performance_report(self, df: pd.DataFrame) -> str:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ HTML ìƒì„±"""
        html = f"""
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ìë™ë§¤ë§¤ ë¶„ì„ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background-color: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }}
        h2 {{ color: #34495e; margin-top: 30px; }}
        .summary {{ background-color: #ecf0f1; padding: 20px; border-radius: 5px; margin: 20px 0; }}
        .metric-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin: 20px 0; }}
        .metric-card {{ background-color: #fff; padding: 15px; border-radius: 5px; border-left: 4px solid #3498db; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        .metric-value {{ font-size: 24px; font-weight: bold; color: #2c3e50; }}
        .metric-label {{ font-size: 14px; color: #7f8c8d; }}
        .improvement {{ color: #27ae60; font-weight: bold; }}
        .regression {{ color: #e74c3c; font-weight: bold; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background-color: #f2f2f2; }}
        .original {{ background-color: #f8f9fa; }}
        .optimized {{ background-color: #e8f5e8; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸš€ ìë™ë§¤ë§¤ ë¶„ì„ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸</h1>
        <p><strong>ìƒì„± ì‹œê°„:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        
        <h2>ğŸ“Š ì„±ëŠ¥ ìš”ì•½</h2>
        <div class="summary">
"""
        
        # ì„±ëŠ¥ ê°œì„  í†µê³„ ê³„ì‚°
        original_results = df[df['version'] == 'original']
        optimized_results = df[df['version'] == 'optimized']
        
        if not original_results.empty and not optimized_results.empty:
            # í‰ê·  ì„±ëŠ¥ ë¹„êµ
            avg_time_original = original_results['execution_time'].mean()
            avg_time_optimized = optimized_results['execution_time'].mean()
            time_improvement = ((avg_time_original - avg_time_optimized) / avg_time_original) * 100
            
            avg_memory_original = original_results['memory_peak_mb'].mean()
            avg_memory_optimized = optimized_results['memory_peak_mb'].mean()
            memory_improvement = ((avg_memory_original - avg_memory_optimized) / avg_memory_original) * 100
            
            html += f"""
            <div class="metric-grid">
                <div class="metric-card">
                    <div class="metric-value">{time_improvement:+.1f}%</div>
                    <div class="metric-label">ì‹¤í–‰ ì‹œê°„ ê°œì„ </div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{memory_improvement:+.1f}%</div>
                    <div class="metric-label">ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°œì„ </div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{len(df[df['success']])}</div>
                    <div class="metric-label">ì„±ê³µí•œ í…ŒìŠ¤íŠ¸</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{len(df[df['success'] == False])}</div>
                    <div class="metric-label">ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸</div>
                </div>
            </div>
"""
        
        html += """
        </div>
        
        <h2>ğŸ“ˆ ìƒì„¸ ê²°ê³¼</h2>
        <table>
            <tr>
                <th>í…ŒìŠ¤íŠ¸</th>
                <th>ë²„ì „</th>
                <th>ë°ì´í„° í¬ê¸°</th>
                <th>ì‹¤í–‰ ì‹œê°„(ì´ˆ)</th>
                <th>ìµœëŒ€ ë©”ëª¨ë¦¬(MB)</th>
                <th>í‰ê·  ë©”ëª¨ë¦¬(MB)</th>
                <th>CPU ì‚¬ìš©ë¥ (%)</th>
                <th>ì„±ê³µ</th>
            </tr>
"""
        
        # ê²°ê³¼ í…Œì´ë¸” ìƒì„±
        for _, row in df.iterrows():
            version_class = "original" if row['version'] == 'original' else "optimized"
            success_text = "âœ…" if row['success'] else "âŒ"
            
            html += f"""
            <tr class="{version_class}">
                <td>{row['test_name']}</td>
                <td>{row['version']}</td>
                <td>{row['data_size']:,}</td>
                <td>{row['execution_time']:.3f}</td>
                <td>{row['memory_peak_mb']:.1f}</td>
                <td>{row['memory_avg_mb']:.1f}</td>
                <td>{row['cpu_usage_avg']:.1f}</td>
                <td>{success_text}</td>
            </tr>
"""
        
        html += """
        </table>
        
        <h2>ğŸ¯ ê²°ë¡ </h2>
        <div class="summary">
"""
        
        if time_improvement > 0:
            html += f"<p class='improvement'>âœ… ìµœì í™”ëœ ë²„ì „ì´ í‰ê·  {time_improvement:.1f}% ë¹ ë¥¸ ì‹¤í–‰ ì‹œê°„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.</p>"
        else:
            html += f"<p class='regression'>âš ï¸ ìµœì í™”ëœ ë²„ì „ì´ í‰ê·  {abs(time_improvement):.1f}% ëŠë¦° ì‹¤í–‰ ì‹œê°„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.</p>"
        
        if memory_improvement > 0:
            html += f"<p class='improvement'>âœ… ìµœì í™”ëœ ë²„ì „ì´ í‰ê·  {memory_improvement:.1f}% ì ì€ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.</p>"
        else:
            html += f"<p class='regression'>âš ï¸ ìµœì í™”ëœ ë²„ì „ì´ í‰ê·  {abs(memory_improvement):.1f}% ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.</p>"
        
        html += """
        </div>
        
        <footer style="margin-top: 40px; padding-top: 20px; border-top: 1px solid #ddd; color: #7f8c8d; text-align: center;">
            <p>ìë™ë§¤ë§¤ ë¶„ì„ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ v1.0</p>
        </footer>
    </div>
</body>
</html>
"""
        
        return html

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ë²¤ì¹˜ë§ˆí¬ ì„¤ì •
    config = BenchmarkConfig(
        test_data_sizes=[1000, 5000, 10000],  # ì‘ì€ í¬ê¸°ë¶€í„° ì‹œì‘
        iterations=1,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 1íšŒë§Œ
        memory_limit_mb=800,
        timeout_seconds=180
    )
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = PerformanceBenchmark(config)
    results = benchmark.run_comprehensive_benchmark()
    
    print("=== ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ ===")
    print(f"ì´ í…ŒìŠ¤íŠ¸ ìˆ˜: {len(results['results'])}")
    
    # ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ ìˆ˜
    successful_tests = [r for r in results['results'] if r.get('success', False)]
    print(f"ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {len(successful_tests)}")
    
    # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ìˆ˜
    failed_tests = [r for r in results['results'] if not r.get('success', False)]
    print(f"ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸: {len(failed_tests)}")
    
    if failed_tests:
        print("\nì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸:")
        for test in failed_tests:
            print(f"- {test['test_name']} ({test['version']}): {test.get('error_message', 'Unknown error')}")
    
    print(f"\nê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {benchmark.results_dir}")
    print("ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")














