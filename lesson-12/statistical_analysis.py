#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìë™ë§¤ë§¤ ë°ì´í„° í†µê³„ ë¶„ì„ ëª¨ë“ˆ
scipyë¥¼ í™œìš©í•œ ê³ ê¸‰ í†µê³„ ë¶„ì„ ë° ê²€ì •
"""

import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import normaltest, shapiro, jarque_bera, kstest
from scipy.stats import ttest_1samp, ttest_ind, mannwhitneyu, wilcoxon
from scipy.stats import chi2_contingency, pearsonr, spearmanr, kendalltau
from scipy.stats import norm, t, chi2
from scipy.optimize import minimize
from typing import Dict, List, Optional, Tuple, Union, Any
from datetime import datetime, timedelta
import logging
from dataclasses import dataclass

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class StatisticalTestResult:
    """í†µê³„ ê²€ì • ê²°ê³¼ í´ë˜ìŠ¤"""
    test_name: str
    statistic: float
    p_value: float
    critical_value: Optional[float] = None
    conclusion: str = ""
    interpretation: str = ""

class NormalityTester:
    """ì •ê·œì„± ê²€ì • í´ë˜ìŠ¤"""
    
    @staticmethod
    def test_normality(data: pd.Series, alpha: float = 0.05) -> List[StatisticalTestResult]:
        """ì •ê·œì„± ê²€ì • ìˆ˜í–‰"""
        results = []
        
        if data.empty:
            logger.warning("ì •ê·œì„± ê²€ì •ì„ ìœ„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return results
        
        # 1. Shapiro-Wilk ê²€ì • (ìƒ˜í”Œ í¬ê¸° <= 5000)
        if len(data) <= 5000:
            try:
                statistic, p_value = shapiro(data)
                result = StatisticalTestResult(
                    test_name="Shapiro-Wilk",
                    statistic=statistic,
                    p_value=p_value,
                    conclusion="ì •ê·œë¶„í¬" if p_value > alpha else "ì •ê·œë¶„í¬ ì•„ë‹˜",
                    interpretation=f"p-value: {p_value:.4f}, Î±: {alpha}"
                )
                results.append(result)
            except Exception as e:
                logger.error(f"Shapiro-Wilk ê²€ì • ì˜¤ë¥˜: {e}")
        
        # 2. Jarque-Bera ê²€ì •
        try:
            statistic, p_value = jarque_bera(data)
            result = StatisticalTestResult(
                test_name="Jarque-Bera",
                statistic=statistic,
                p_value=p_value,
                conclusion="ì •ê·œë¶„í¬" if p_value > alpha else "ì •ê·œë¶„í¬ ì•„ë‹˜",
                interpretation=f"p-value: {p_value:.4f}, Î±: {alpha}"
            )
            results.append(result)
        except Exception as e:
            logger.error(f"Jarque-Bera ê²€ì • ì˜¤ë¥˜: {e}")
        
        # 3. Kolmogorov-Smirnov ê²€ì •
        try:
            statistic, p_value = kstest(data, 'norm', args=(data.mean(), data.std()))
            result = StatisticalTestResult(
                test_name="Kolmogorov-Smirnov",
                statistic=statistic,
                p_value=p_value,
                conclusion="ì •ê·œë¶„í¬" if p_value > alpha else "ì •ê·œë¶„í¬ ì•„ë‹˜",
                interpretation=f"p-value: {p_value:.4f}, Î±: {alpha}"
            )
            results.append(result)
        except Exception as e:
            logger.error(f"Kolmogorov-Smirnov ê²€ì • ì˜¤ë¥˜: {e}")
        
        return results

class CorrelationAnalyzer:
    """ìƒê´€ê´€ê³„ ë¶„ì„ í´ë˜ìŠ¤"""
    
    @staticmethod
    def calculate_correlations(data1: pd.Series, data2: pd.Series) -> Dict[str, Any]:
        """ë‹¤ì–‘í•œ ìƒê´€ê´€ê³„ ê³„ìˆ˜ ê³„ì‚°"""
        results = {}
        
        if data1.empty or data2.empty:
            logger.warning("ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return results
        
        # ê³µí†µ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
        common_index = data1.index.intersection(data2.index)
        if len(common_index) == 0:
            logger.warning("ê³µí†µ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return results
        
        x = data1.loc[common_index]
        y = data2.loc[common_index]
        
        # 1. í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜
        try:
            pearson_corr, pearson_p = pearsonr(x, y)
            results['pearson'] = {
                'correlation': pearson_corr,
                'p_value': pearson_p,
                'strength': CorrelationAnalyzer._interpret_correlation(pearson_corr)
            }
        except Exception as e:
            logger.error(f"í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
        
        # 2. ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜
        try:
            spearman_corr, spearman_p = spearmanr(x, y)
            results['spearman'] = {
                'correlation': spearman_corr,
                'p_value': spearman_p,
                'strength': CorrelationAnalyzer._interpret_correlation(spearman_corr)
            }
        except Exception as e:
            logger.error(f"ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
        
        # 3. ì¼„ë‹¬ íƒ€ìš°
        try:
            kendall_corr, kendall_p = kendalltau(x, y)
            results['kendall'] = {
                'correlation': kendall_corr,
                'p_value': kendall_p,
                'strength': CorrelationAnalyzer._interpret_correlation(kendall_corr)
            }
        except Exception as e:
            logger.error(f"ì¼„ë‹¬ íƒ€ìš° ê³„ì‚° ì˜¤ë¥˜: {e}")
        
        return results
    
    @staticmethod
    def _interpret_correlation(corr: float) -> str:
        """ìƒê´€ê³„ìˆ˜ ê°•ë„ í•´ì„"""
        abs_corr = abs(corr)
        if abs_corr >= 0.9:
            return "ë§¤ìš° ê°•í•¨"
        elif abs_corr >= 0.7:
            return "ê°•í•¨"
        elif abs_corr >= 0.5:
            return "ì¤‘ê°„"
        elif abs_corr >= 0.3:
            return "ì•½í•¨"
        else:
            return "ë§¤ìš° ì•½í•¨"

class HypothesisTester:
    """ê°€ì„¤ ê²€ì • í´ë˜ìŠ¤"""
    
    @staticmethod
    def test_mean_difference(data1: pd.Series, data2: pd.Series, 
                           alpha: float = 0.05) -> List[StatisticalTestResult]:
        """í‰ê·  ì°¨ì´ ê²€ì •"""
        results = []
        
        if data1.empty or data2.empty:
            logger.warning("í‰ê·  ì°¨ì´ ê²€ì •ì„ ìœ„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return results
        
        # ê³µí†µ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
        common_index = data1.index.intersection(data2.index)
        if len(common_index) == 0:
            logger.warning("ê³µí†µ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return results
        
        x = data1.loc[common_index]
        y = data2.loc[common_index]
        
        # 1. ì •ê·œì„± ê²€ì •
        norm_tests_x = NormalityTester.test_normality(x, alpha)
        norm_tests_y = NormalityTester.test_normality(y, alpha)
        
        # 2. ë…ë¦½ í‘œë³¸ t-ê²€ì • (ì •ê·œë¶„í¬ì¸ ê²½ìš°)
        try:
            if any(test.p_value > alpha for test in norm_tests_x) and \
               any(test.p_value > alpha for test in norm_tests_y):
                
                t_stat, p_value = ttest_ind(x, y)
                result = StatisticalTestResult(
                    test_name="Independent t-test",
                    statistic=t_stat,
                    p_value=p_value,
                    conclusion="í‰ê· ì´ ë‹¤ë¦„" if p_value < alpha else "í‰ê· ì´ ê°™ìŒ",
                    interpretation=f"t-statistic: {t_stat:.4f}, p-value: {p_value:.4f}"
                )
                results.append(result)
        except Exception as e:
            logger.error(f"ë…ë¦½ í‘œë³¸ t-ê²€ì • ì˜¤ë¥˜: {e}")
        
        # 3. Mann-Whitney U ê²€ì • (ë¹„ëª¨ìˆ˜ ê²€ì •)
        try:
            u_stat, p_value = mannwhitneyu(x, y, alternative='two-sided')
            result = StatisticalTestResult(
                test_name="Mann-Whitney U test",
                statistic=u_stat,
                p_value=p_value,
                conclusion="ë¶„í¬ê°€ ë‹¤ë¦„" if p_value < alpha else "ë¶„í¬ê°€ ê°™ìŒ",
                interpretation=f"U-statistic: {u_stat:.4f}, p-value: {p_value:.4f}"
            )
            results.append(result)
        except Exception as e:
            logger.error(f"Mann-Whitney U ê²€ì • ì˜¤ë¥˜: {e}")
        
        return results
    
    @staticmethod
    def test_single_mean(data: pd.Series, expected_mean: float, 
                        alpha: float = 0.05) -> StatisticalTestResult:
        """ë‹¨ì¼ í‰ê·  ê²€ì •"""
        if data.empty:
            logger.warning("ë‹¨ì¼ í‰ê·  ê²€ì •ì„ ìœ„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return StatisticalTestResult("", 0, 1, conclusion="ë°ì´í„° ì—†ìŒ")
        
        try:
            t_stat, p_value = ttest_1samp(data, expected_mean)
            result = StatisticalTestResult(
                test_name="One-sample t-test",
                statistic=t_stat,
                p_value=p_value,
                conclusion="ê¸°ëŒ“ê°’ê³¼ ë‹¤ë¦„" if p_value < alpha else "ê¸°ëŒ“ê°’ê³¼ ê°™ìŒ",
                interpretation=f"expected mean: {expected_mean:.4f}, "
                             f"sample mean: {data.mean():.4f}, "
                             f"t-statistic: {t_stat:.4f}, p-value: {p_value:.4f}"
            )
            return result
        except Exception as e:
            logger.error(f"ë‹¨ì¼ í‰ê·  ê²€ì • ì˜¤ë¥˜: {e}")
            return StatisticalTestResult("", 0, 1, conclusion="ì˜¤ë¥˜ ë°œìƒ")

class RiskStatisticsCalculator:
    """ë¦¬ìŠ¤í¬ í†µê³„ ê³„ì‚° í´ë˜ìŠ¤"""
    
    @staticmethod
    def calculate_var_cvar(returns: pd.Series, confidence_levels: List[float] = [0.95, 0.99]) -> Dict[str, Dict]:
        """VaR ë° CVaR ê³„ì‚°"""
        results = {}
        
        if returns.empty:
            logger.warning("VaR/CVaR ê³„ì‚°ì„ ìœ„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return results
        
        for confidence in confidence_levels:
            alpha = 1 - confidence
            
            # 1. íˆìŠ¤í† ë¦¬ì»¬ VaR/CVaR
            var_hist = np.percentile(returns, alpha * 100)
            cvar_hist = returns[returns <= var_hist].mean()
            
            # 2. íŒŒë¼ë©”íŠ¸ë¦­ VaR/CVaR (ì •ê·œë¶„í¬ ê°€ì •)
            mean_return = returns.mean()
            std_return = returns.std()
            var_param = norm.ppf(alpha, mean_return, std_return)
            cvar_param = mean_return - std_return * norm.pdf(norm.ppf(alpha)) / alpha
            
            # 3. ëª¨ë©˜íŠ¸ VaR/CVaR (ì™œë„, ì²¨ë„ ê³ ë ¤)
            skewness = stats.skew(returns)
            kurtosis = stats.kurtosis(returns)
            
            # Cornish-Fisher í™•ì¥
            z_alpha = norm.ppf(alpha)
            z_cf = z_alpha + (z_alpha**2 - 1) * skewness / 6 + \
                   (z_alpha**3 - 3*z_alpha) * kurtosis / 24 - \
                   (2*z_alpha**3 - 5*z_alpha) * skewness**2 / 36
            
            var_cf = mean_return + std_return * z_cf
            cvar_cf = mean_return - std_return * (norm.pdf(z_cf) / alpha)
            
            results[f"{confidence*100:.0f}%"] = {
                'historical': {
                    'var': var_hist,
                    'cvar': cvar_hist
                },
                'parametric': {
                    'var': var_param,
                    'cvar': cvar_param
                },
                'cornish_fisher': {
                    'var': var_cf,
                    'cvar': cvar_cf
                },
                'statistics': {
                    'mean': mean_return,
                    'std': std_return,
                    'skewness': skewness,
                    'kurtosis': kurtosis
                }
            }
        
        return results
    
    @staticmethod
    def calculate_risk_metrics(returns: pd.Series) -> Dict[str, float]:
        """ë¦¬ìŠ¤í¬ ì§€í‘œ ê³„ì‚°"""
        if returns.empty:
            logger.warning("ë¦¬ìŠ¤í¬ ì§€í‘œ ê³„ì‚°ì„ ìœ„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {}
        
        # ê¸°ë³¸ í†µê³„
        mean_return = returns.mean()
        std_return = returns.std()
        
        # ê³ ì°¨ ëª¨ë©˜íŠ¸
        skewness = stats.skew(returns)
        kurtosis = stats.kurtosis(returns)
        
        # VaR ë° CVaR
        var_95 = np.percentile(returns, 5)
        cvar_95 = returns[returns <= var_95].mean()
        var_99 = np.percentile(returns, 1)
        cvar_99 = returns[returns <= var_99].mean()
        
        # ìµœëŒ€ ë‚™í­
        cumulative_returns = (1 + returns).cumprod()
        running_max = cumulative_returns.expanding().max()
        drawdown = (cumulative_returns - running_max) / running_max
        max_drawdown = drawdown.min()
        
        # ìƒ¤í”„ ë¹„ìœ¨ (ë¬´ìœ„í—˜ ìˆ˜ìµë¥  2% ê°€ì •)
        risk_free_rate = 0.02 / 252  # ì¼ì¼ ë¬´ìœ„í—˜ ìˆ˜ìµë¥ 
        sharpe_ratio = (mean_return - risk_free_rate) / std_return * np.sqrt(252)
        
        # ì†Œë¥´í‹°ë…¸ ë¹„ìœ¨
        downside_returns = returns[returns < 0]
        downside_std = downside_returns.std() if len(downside_returns) > 0 else 0
        sortino_ratio = (mean_return - risk_free_rate) / downside_std * np.sqrt(252) if downside_std > 0 else 0
        
        # ì¹¼ë§ˆ ë¹„ìœ¨
        calmar_ratio = (mean_return * 252) / abs(max_drawdown) if max_drawdown != 0 else 0
        
        return {
            'mean_return': mean_return,
            'std_return': std_return,
            'skewness': skewness,
            'kurtosis': kurtosis,
            'var_95': var_95,
            'cvar_95': cvar_95,
            'var_99': var_99,
            'cvar_99': cvar_99,
            'max_drawdown': max_drawdown,
            'sharpe_ratio': sharpe_ratio,
            'sortino_ratio': sortino_ratio,
            'calmar_ratio': calmar_ratio
        }

class TimeSeriesAnalyzer:
    """ì‹œê³„ì—´ ë¶„ì„ í´ë˜ìŠ¤"""
    
    @staticmethod
    def test_stationarity(data: pd.Series, alpha: float = 0.05) -> StatisticalTestResult:
        """ì •ìƒì„± ê²€ì • (ADF í…ŒìŠ¤íŠ¸)"""
        if data.empty:
            logger.warning("ì •ìƒì„± ê²€ì •ì„ ìœ„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return StatisticalTestResult("", 0, 1, conclusion="ë°ì´í„° ì—†ìŒ")
        
        try:
            from statsmodels.tsa.stattools import adfuller
            
            result = adfuller(data.dropna())
            adf_statistic = result[0]
            p_value = result[1]
            critical_values = result[4]
            
            conclusion = "ì •ìƒì„±" if p_value < alpha else "ë¹„ì •ìƒì„±"
            
            return StatisticalTestResult(
                test_name="Augmented Dickey-Fuller",
                statistic=adf_statistic,
                p_value=p_value,
                critical_value=critical_values['5%'],
                conclusion=conclusion,
                interpretation=f"ADF statistic: {adf_statistic:.4f}, p-value: {p_value:.4f}"
            )
        except ImportError:
            logger.warning("statsmodelsê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ADF í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return StatisticalTestResult("", 0, 1, conclusion="ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
        except Exception as e:
            logger.error(f"ì •ìƒì„± ê²€ì • ì˜¤ë¥˜: {e}")
            return StatisticalTestResult("", 0, 1, conclusion="ì˜¤ë¥˜ ë°œìƒ")
    
    @staticmethod
    def detect_autocorrelation(data: pd.Series, lags: int = 10) -> Dict[str, float]:
        """ìê¸°ìƒê´€ ê²€ì •"""
        if data.empty:
            logger.warning("ìê¸°ìƒê´€ ê²€ì •ì„ ìœ„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {}
        
        try:
            from statsmodels.tsa.stattools import acf, pacf
            
            # ìê¸°ìƒê´€í•¨ìˆ˜ (ACF)
            acf_values = acf(data.dropna(), nlags=lags, fft=False)
            
            # ë¶€ë¶„ìê¸°ìƒê´€í•¨ìˆ˜ (PACF)
            pacf_values = pacf(data.dropna(), nlags=lags)
            
            results = {}
            for i in range(1, lags + 1):
                results[f'acf_lag_{i}'] = acf_values[i]
                results[f'pacf_lag_{i}'] = pacf_values[i]
            
            return results
        except ImportError:
            logger.warning("statsmodelsê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ìê¸°ìƒê´€ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return {}
        except Exception as e:
            logger.error(f"ìê¸°ìƒê´€ ê²€ì • ì˜¤ë¥˜: {e}")
            return {}

class StatisticalAnalyzer:
    """ì¢…í•© í†µê³„ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def comprehensive_analysis(self, trades_df: pd.DataFrame, 
                             account_df: pd.DataFrame) -> Dict[str, Any]:
        """ì¢…í•© í†µê³„ ë¶„ì„ ìˆ˜í–‰"""
        results = {
            'timestamp': datetime.now().isoformat(),
            'data_summary': {},
            'normality_tests': {},
            'correlation_analysis': {},
            'risk_statistics': {},
            'time_series_analysis': {},
            'hypothesis_tests': {}
        }
        
        try:
            # ë°ì´í„° ìš”ì•½
            if not account_df.empty and 'daily_return_pct' in account_df.columns:
                daily_returns = account_df['daily_return_pct'].dropna()
                
                results['data_summary'] = {
                    'total_observations': len(daily_returns),
                    'mean': daily_returns.mean(),
                    'std': daily_returns.std(),
                    'min': daily_returns.min(),
                    'max': daily_returns.max(),
                    'skewness': stats.skew(daily_returns),
                    'kurtosis': stats.kurtosis(daily_returns)
                }
                
                # ì •ê·œì„± ê²€ì •
                normality_results = NormalityTester.test_normality(daily_returns)
                results['normality_tests']['daily_returns'] = [
                    {
                        'test_name': test.test_name,
                        'statistic': test.statistic,
                        'p_value': test.p_value,
                        'conclusion': test.conclusion
                    }
                    for test in normality_results
                ]
                
                # ë¦¬ìŠ¤í¬ í†µê³„
                results['risk_statistics'] = RiskStatisticsCalculator.calculate_risk_metrics(daily_returns)
                
                # VaR/CVaR ë¶„ì„
                results['var_cvar_analysis'] = RiskStatisticsCalculator.calculate_var_cvar(daily_returns)
                
                # ì‹œê³„ì—´ ë¶„ì„
                stationarity_result = TimeSeriesAnalyzer.test_stationarity(daily_returns)
                results['time_series_analysis']['stationarity'] = {
                    'test_name': stationarity_result.test_name,
                    'statistic': stationarity_result.statistic,
                    'p_value': stationarity_result.p_value,
                    'conclusion': stationarity_result.conclusion
                }
                
                # ìê¸°ìƒê´€ ë¶„ì„
                autocorr_results = TimeSeriesAnalyzer.detect_autocorrelation(daily_returns)
                results['time_series_analysis']['autocorrelation'] = autocorr_results
            
            # ê±°ë˜ ë°ì´í„° ë¶„ì„
            if not trades_df.empty:
                sell_trades = trades_df[trades_df['side'] == 'SELL']
                if not sell_trades.empty:
                    pnl_data = sell_trades['pnl'].dropna()
                    
                    # P&L ì •ê·œì„± ê²€ì •
                    pnl_normality = NormalityTester.test_normality(pnl_data)
                    results['normality_tests']['pnl'] = [
                        {
                            'test_name': test.test_name,
                            'statistic': test.statistic,
                            'p_value': test.p_value,
                            'conclusion': test.conclusion
                        }
                        for test in pnl_normality
                    ]
                    
                    # P&L í‰ê· ì´ 0ê³¼ ë‹¤ë¥¸ì§€ ê²€ì •
                    pnl_mean_test = HypothesisTester.test_single_mean(pnl_data, 0)
                    results['hypothesis_tests']['pnl_mean'] = {
                        'test_name': pnl_mean_test.test_name,
                        'statistic': pnl_mean_test.statistic,
                        'p_value': pnl_mean_test.p_value,
                        'conclusion': pnl_mean_test.conclusion
                    }
            
            # ì‹¬ë³¼ë³„ ìƒê´€ê´€ê³„ ë¶„ì„
            if not trades_df.empty:
                symbol_analysis = {}
                symbols = trades_df['symbol'].unique()
                
                for symbol in symbols:
                    symbol_trades = trades_df[trades_df['symbol'] == symbol]
                    if len(symbol_trades) > 10:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                        symbol_returns = symbol_trades['pnl'].dropna()
                        if len(symbol_returns) > 5:
                            symbol_analysis[symbol] = {
                                'count': len(symbol_returns),
                                'mean': symbol_returns.mean(),
                                'std': symbol_returns.std(),
                                'normality': [
                                    {
                                        'test_name': test.test_name,
                                        'p_value': test.p_value,
                                        'conclusion': test.conclusion
                                    }
                                    for test in NormalityTester.test_normality(symbol_returns)
                                ]
                            }
                
                results['symbol_analysis'] = symbol_analysis
            
            self.logger.info("ì¢…í•© í†µê³„ ë¶„ì„ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"í†µê³„ ë¶„ì„ ì˜¤ë¥˜: {e}")
            results['error'] = str(e)
        
        return results
    
    def generate_statistical_report(self, analysis_results: Dict[str, Any]) -> str:
        """í†µê³„ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = f"""
=== ìë™ë§¤ë§¤ í†µê³„ ë¶„ì„ ë¦¬í¬íŠ¸ ===
ìƒì„± ì‹œê°„: {analysis_results.get('timestamp', 'N/A')}

ğŸ“Š ë°ì´í„° ìš”ì•½
"""
        
        data_summary = analysis_results.get('data_summary', {})
        if data_summary:
            report += f"""
- ê´€ì¸¡ì¹˜ ìˆ˜: {data_summary.get('total_observations', 0):,}ê°œ
- í‰ê· : {data_summary.get('mean', 0):.4f}%
- í‘œì¤€í¸ì°¨: {data_summary.get('std', 0):.4f}%
- ìµœì†Œê°’: {data_summary.get('min', 0):.4f}%
- ìµœëŒ€ê°’: {data_summary.get('max', 0):.4f}%
- ì™œë„: {data_summary.get('skewness', 0):.4f}
- ì²¨ë„: {data_summary.get('kurtosis', 0):.4f}
"""
        
        # ì •ê·œì„± ê²€ì • ê²°ê³¼
        normality_tests = analysis_results.get('normality_tests', {})
        if normality_tests:
            report += "\nğŸ” ì •ê·œì„± ê²€ì • ê²°ê³¼\n"
            for data_type, tests in normality_tests.items():
                report += f"\n{data_type.upper()}:\n"
                for test in tests:
                    report += f"- {test['test_name']}: {test['conclusion']} (p-value: {test['p_value']:.4f})\n"
        
        # ë¦¬ìŠ¤í¬ í†µê³„
        risk_stats = analysis_results.get('risk_statistics', {})
        if risk_stats:
            report += "\nâš ï¸ ë¦¬ìŠ¤í¬ í†µê³„\n"
            report += f"- ìƒ¤í”„ ë¹„ìœ¨: {risk_stats.get('sharpe_ratio', 0):.4f}\n"
            report += f"- ì†Œë¥´í‹°ë…¸ ë¹„ìœ¨: {risk_stats.get('sortino_ratio', 0):.4f}\n"
            report += f"- ì¹¼ë§ˆ ë¹„ìœ¨: {risk_stats.get('calmar_ratio', 0):.4f}\n"
            report += f"- ìµœëŒ€ ë‚™í­: {risk_stats.get('max_drawdown', 0):.4f}\n"
            report += f"- VaR (95%): {risk_stats.get('var_95', 0):.4f}\n"
            report += f"- CVaR (95%): {risk_stats.get('cvar_95', 0):.4f}\n"
        
        # ì‹œê³„ì—´ ë¶„ì„
        ts_analysis = analysis_results.get('time_series_analysis', {})
        if ts_analysis:
            report += "\nğŸ“ˆ ì‹œê³„ì—´ ë¶„ì„\n"
            stationarity = ts_analysis.get('stationarity', {})
            if stationarity:
                report += f"- ì •ìƒì„±: {stationarity.get('conclusion', 'N/A')} (ADF p-value: {stationarity.get('p_value', 0):.4f})\n"
        
        # ê°€ì„¤ ê²€ì •
        hypothesis_tests = analysis_results.get('hypothesis_tests', {})
        if hypothesis_tests:
            report += "\nğŸ§ª ê°€ì„¤ ê²€ì •\n"
            for test_name, test_result in hypothesis_tests.items():
                report += f"- {test_result.get('test_name', test_name)}: {test_result.get('conclusion', 'N/A')} (p-value: {test_result.get('p_value', 0):.4f})\n"
        
        return report.strip()

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    from data_processor import TradingDataProcessor, DataConfig
    
    # ì„¤ì • ë° ë°ì´í„° ë¡œë“œ
    config = DataConfig()
    processor = TradingDataProcessor(config)
    
    trades = processor.load_trade_data()
    account = processor.load_account_history()
    
    if not trades.empty and not account.empty:
        # ë°ì´í„° ì „ì²˜ë¦¬
        processed_trades = processor.preprocess_trade_data(trades)
        processed_account = processor.preprocess_account_data(account)
        
        # í†µê³„ ë¶„ì„
        analyzer = StatisticalAnalyzer()
        analysis_results = analyzer.comprehensive_analysis(processed_trades, processed_account)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = analyzer.generate_statistical_report(analysis_results)
        print(report)
        
        # ìƒì„¸ ê²°ê³¼ ì¶œë ¥
        print("\n=== ìƒì„¸ ë¶„ì„ ê²°ê³¼ ===")
        for key, value in analysis_results.items():
            if key != 'timestamp':
                print(f"\n{key}:")
                print(value)
    else:
        print("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

