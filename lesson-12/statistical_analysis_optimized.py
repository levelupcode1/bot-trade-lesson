#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìë™ë§¤ë§¤ í†µê³„ ë¶„ì„ ëª¨ë“ˆ (ìµœì í™” ë²„ì „)
ë²¡í„°í™” ì—°ì‚°ê³¼ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ìµœì í™”ëœ í†µê³„ ë¶„ì„
"""

import pandas as pd
import numpy as np
import scipy.stats as stats
from scipy import signal
from numba import jit, prange
import gc
from typing import Dict, List, Optional, Tuple, Union, Any
from datetime import datetime, timedelta
import logging
from dataclasses import dataclass
from functools import lru_cache
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import threading
from statsmodels.stats.diagnostic import acorr_ljungbox
from statsmodels.tsa.stattools import adfuller
import warnings

# ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
warnings.filterwarnings('ignore', category=RuntimeWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class StatisticalResults:
    """í†µê³„ ë¶„ì„ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    # ê¸°ë³¸ í†µê³„
    count: int = 0
    mean: float = 0.0
    std: float = 0.0
    min_val: float = 0.0
    max_val: float = 0.0
    skewness: float = 0.0
    kurtosis: float = 0.0
    
    # ì •ê·œì„± ê²€ì •
    shapiro_wilk: Dict[str, float] = None
    jarque_bera: Dict[str, float] = None
    kolmogorov_smirnov: Dict[str, float] = None
    
    # VaR/CVaR
    var_95: float = 0.0
    var_99: float = 0.0
    cvar_95: float = 0.0
    cvar_99: float = 0.0
    
    # ìƒê´€ê´€ê³„
    pearson_corr: Dict[str, float] = None
    spearman_corr: Dict[str, float] = None
    
    # ì‹œê³„ì—´ ë¶„ì„
    adf_test: Dict[str, float] = None
    ljung_box: Dict[str, float] = None
    
    # ê°€ì„¤ ê²€ì •
    t_test: Dict[str, float] = None

# Numbaë¥¼ ì‚¬ìš©í•œ ê³ ì„±ëŠ¥ í†µê³„ ê³„ì‚° í•¨ìˆ˜ë“¤
@jit(nopython=True, cache=True)
def calculate_skewness_kurtosis_numba(data: np.ndarray) -> Tuple[float, float]:
    """Numbaë¥¼ ì‚¬ìš©í•œ ì™œë„ì™€ ì²¨ë„ ê³„ì‚°"""
    n = len(data)
    if n < 3:
        return 0.0, 0.0
    
    mean_val = np.mean(data)
    std_val = np.std(data)
    
    if std_val == 0:
        return 0.0, 0.0
    
    # í‘œì¤€í™”ëœ ë°ì´í„°
    standardized = (data - mean_val) / std_val
    
    # ì™œë„ ê³„ì‚°
    skewness = np.mean(standardized ** 3)
    
    # ì²¨ë„ ê³„ì‚° (ì •ê·œë¶„í¬ ëŒ€ë¹„)
    kurtosis = np.mean(standardized ** 4) - 3
    
    return skewness, kurtosis

@jit(nopython=True, parallel=True, cache=True)
def calculate_rolling_statistics_numba(data: np.ndarray, window: int) -> Tuple[np.ndarray, np.ndarray]:
    """Numbaë¥¼ ì‚¬ìš©í•œ ë¡¤ë§ í†µê³„ ê³„ì‚°"""
    n = len(data)
    if n < window:
        return np.zeros(n), np.zeros(n)
    
    rolling_mean = np.zeros(n)
    rolling_std = np.zeros(n)
    
    for i in prange(window - 1, n):
        start_idx = i - window + 1
        window_data = data[start_idx:i+1]
        
        rolling_mean[i] = np.mean(window_data)
        rolling_std[i] = np.std(window_data)
    
    return rolling_mean, rolling_std

@jit(nopython=True, cache=True)
def calculate_var_cvar_historical_numba(data: np.ndarray, confidence: float) -> Tuple[float, float]:
    """Numbaë¥¼ ì‚¬ìš©í•œ íˆìŠ¤í† ë¦¬ì»¬ VaR/CVaR ê³„ì‚°"""
    if len(data) == 0:
        return 0.0, 0.0
    
    sorted_data = np.sort(data)
    var_index = int((1 - confidence) * len(sorted_data))
    var_value = sorted_data[var_index]
    
    # CVaR ê³„ì‚°
    cvar_data = sorted_data[:var_index+1]
    cvar_value = np.mean(cvar_data) if len(cvar_data) > 0 else 0.0
    
    return var_value, cvar_value

@jit(nopython=True, cache=True)
def calculate_autocorrelation_numba(data: np.ndarray, max_lag: int) -> np.ndarray:
    """Numbaë¥¼ ì‚¬ìš©í•œ ìê¸°ìƒê´€ ê³„ì‚°"""
    n = len(data)
    if n < 2:
        return np.array([])
    
    max_lag = min(max_lag, n - 1)
    autocorr = np.zeros(max_lag + 1)
    
    # í‰ê·  ì œê±°
    mean_val = np.mean(data)
    centered_data = data - mean_val
    
    # ë¶„ì‚° ê³„ì‚°
    variance = np.sum(centered_data ** 2)
    
    if variance == 0:
        return np.zeros(max_lag + 1)
    
    # ìê¸°ìƒê´€ ê³„ì‚°
    for lag in range(max_lag + 1):
        if lag == 0:
            autocorr[lag] = 1.0
        else:
            numerator = np.sum(centered_data[lag:] * centered_data[:n-lag])
            autocorr[lag] = numerator / variance
    
    return autocorr

class OptimizedBasicStatistics:
    """ìµœì í™”ëœ ê¸°ë³¸ í†µê³„ ê³„ì‚° í´ë˜ìŠ¤"""
    
    @staticmethod
    def calculate_descriptive_stats_vectorized(data: np.ndarray) -> Dict[str, float]:
        """ë²¡í„°í™”ëœ ê¸°ìˆ í†µê³„ ê³„ì‚°"""
        if len(data) == 0:
            return {
                'count': 0, 'mean': 0.0, 'std': 0.0, 'min': 0.0, 'max': 0.0,
                'skewness': 0.0, 'kurtosis': 0.0
            }
        
        # ê¸°ë³¸ í†µê³„ (ë²¡í„°í™”)
        count = len(data)
        mean_val = np.mean(data)
        std_val = np.std(data, ddof=1) if count > 1 else 0.0
        min_val = np.min(data)
        max_val = np.max(data)
        
        # ì™œë„ì™€ ì²¨ë„ (Numba ìµœì í™”)
        skewness, kurtosis = calculate_skewness_kurtosis_numba(data)
        
        return {
            'count': count,
            'mean': mean_val,
            'std': std_val,
            'min': min_val,
            'max': max_val,
            'skewness': skewness,
            'kurtosis': kurtosis
        }
    
    @staticmethod
    def calculate_rolling_stats_optimized(data: pd.Series, window: int = 20) -> pd.DataFrame:
        """ìµœì í™”ëœ ë¡¤ë§ í†µê³„ ê³„ì‚°"""
        if len(data) < window:
            return pd.DataFrame()
        
        # Numba ìµœì í™” ì‚¬ìš©
        data_array = data.values.astype(np.float64)
        rolling_mean, rolling_std = calculate_rolling_statistics_numba(data_array, window)
        
        result = pd.DataFrame({
            'rolling_mean': rolling_mean,
            'rolling_std': rolling_std
        }, index=data.index)
        
        return result

class OptimizedNormalityTests:
    """ìµœì í™”ëœ ì •ê·œì„± ê²€ì • í´ë˜ìŠ¤"""
    
    @staticmethod
    def perform_normality_tests_optimized(data: np.ndarray, sample_size: int = 5000) -> Dict[str, Dict[str, float]]:
        """ìµœì í™”ëœ ì •ê·œì„± ê²€ì • ìˆ˜í–‰"""
        if len(data) == 0:
            return {
                'shapiro_wilk': {'statistic': 0.0, 'p_value': 1.0},
                'jarque_bera': {'statistic': 0.0, 'p_value': 1.0},
                'kolmogorov_smirnov': {'statistic': 0.0, 'p_value': 1.0}
            }
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒ˜í”Œë§
        if len(data) > sample_size:
            np.random.seed(42)
            sample_indices = np.random.choice(len(data), sample_size, replace=False)
            sample_data = data[sample_indices]
            logger.info(f"ì •ê·œì„± ê²€ì •ì„ ìœ„í•´ ë°ì´í„° ìƒ˜í”Œë§: {len(data)} -> {len(sample_data)}")
        else:
            sample_data = data
        
        results = {}
        
        try:
            # Shapiro-Wilk ê²€ì • (ìµœëŒ€ 5000ê°œ)
            if len(sample_data) <= 5000:
                shapiro_stat, shapiro_p = stats.shapiro(sample_data)
                results['shapiro_wilk'] = {'statistic': shapiro_stat, 'p_value': shapiro_p}
            else:
                results['shapiro_wilk'] = {'statistic': 0.0, 'p_value': 1.0}
            
            # Jarque-Bera ê²€ì •
            jb_stat, jb_p = stats.jarque_bera(sample_data)
            results['jarque_bera'] = {'statistic': jb_stat, 'p_value': jb_p}
            
            # Kolmogorov-Smirnov ê²€ì •
            ks_stat, ks_p = stats.kstest(sample_data, 'norm', args=(np.mean(sample_data), np.std(sample_data)))
            results['kolmogorov_smirnov'] = {'statistic': ks_stat, 'p_value': ks_p}
            
        except Exception as e:
            logger.error(f"ì •ê·œì„± ê²€ì • ì˜¤ë¥˜: {e}")
            results = {
                'shapiro_wilk': {'statistic': 0.0, 'p_value': 1.0},
                'jarque_bera': {'statistic': 0.0, 'p_value': 1.0},
                'kolmogorov_smirnov': {'statistic': 0.0, 'p_value': 1.0}
            }
        
        return results

class OptimizedRiskAnalysis:
    """ìµœì í™”ëœ ë¦¬ìŠ¤í¬ ë¶„ì„ í´ë˜ìŠ¤"""
    
    @staticmethod
    def calculate_var_cvar_optimized(returns: np.ndarray, confidence_levels: List[float] = None) -> Dict[str, Tuple[float, float]]:
        """ìµœì í™”ëœ VaR/CVaR ê³„ì‚°"""
        if confidence_levels is None:
            confidence_levels = [0.95, 0.99]
        
        results = {}
        
        for confidence in confidence_levels:
            var, cvar = calculate_var_cvar_historical_numba(returns.astype(np.float64), confidence)
            results[f'var_{int(confidence*100)}'] = (var, cvar)
        
        return results
    
    @staticmethod
    def calculate_expected_shortfall_optimized(returns: np.ndarray, confidence: float = 0.95) -> float:
        """ìµœì í™”ëœ ì˜ˆìƒ ë¶€ì¡±ì•¡ ê³„ì‚°"""
        if len(returns) == 0:
            return 0.0
        
        var, cvar = calculate_var_cvar_historical_numba(returns.astype(np.float64), confidence)
        return abs(cvar)

class OptimizedCorrelationAnalysis:
    """ìµœì í™”ëœ ìƒê´€ê´€ê³„ ë¶„ì„ í´ë˜ìŠ¤"""
    
    @staticmethod
    def calculate_correlations_optimized(data_dict: Dict[str, np.ndarray]) -> Dict[str, Dict[str, float]]:
        """ìµœì í™”ëœ ìƒê´€ê´€ê³„ ê³„ì‚°"""
        if len(data_dict) < 2:
            return {}
        
        # ë°ì´í„° ì •ë ¬ ë° ê¸¸ì´ ë§ì¶”ê¸°
        min_length = min(len(data) for data in data_dict.values())
        if min_length == 0:
            return {}
        
        aligned_data = {}
        for key, data in data_dict.items():
            if len(data) > min_length:
                aligned_data[key] = data[-min_length:]
            else:
                aligned_data[key] = data
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(aligned_data)
        
        # NaN ì œê±°
        df_clean = df.dropna()
        
        if len(df_clean) < 2:
            return {}
        
        results = {}
        
        try:
            # Pearson ìƒê´€ê³„ìˆ˜
            pearson_corr = df_clean.corr(method='pearson')
            results['pearson'] = pearson_corr.to_dict()
            
            # Spearman ìƒê´€ê³„ìˆ˜
            spearman_corr = df_clean.corr(method='spearman')
            results['spearman'] = spearman_corr.to_dict()
            
        except Exception as e:
            logger.error(f"ìƒê´€ê´€ê³„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            results = {}
        
        return results

class OptimizedTimeSeriesAnalysis:
    """ìµœì í™”ëœ ì‹œê³„ì—´ ë¶„ì„ í´ë˜ìŠ¤"""
    
    @staticmethod
    def perform_stationarity_test_optimized(data: pd.Series, max_lag: int = None) -> Dict[str, float]:
        """ìµœì í™”ëœ ì •ìƒì„± ê²€ì •"""
        if len(data) < 10:
            return {'adf_statistic': 0.0, 'adf_p_value': 1.0, 'critical_values': {}}
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒ˜í”Œë§
        if len(data) > 10000:
            sample_indices = np.linspace(0, len(data)-1, 10000, dtype=int)
            sample_data = data.iloc[sample_indices]
            logger.info(f"ì •ìƒì„± ê²€ì •ì„ ìœ„í•´ ë°ì´í„° ìƒ˜í”Œë§: {len(data)} -> {len(sample_data)}")
        else:
            sample_data = data
        
        try:
            # ADF ê²€ì •
            adf_result = adfuller(sample_data.dropna(), maxlag=max_lag)
            
            return {
                'adf_statistic': adf_result[0],
                'adf_p_value': adf_result[1],
                'critical_values': {
                    '1%': adf_result[4]['1%'],
                    '5%': adf_result[4]['5%'],
                    '10%': adf_result[4]['10%']
                }
            }
            
        except Exception as e:
            logger.error(f"ì •ìƒì„± ê²€ì • ì˜¤ë¥˜: {e}")
            return {'adf_statistic': 0.0, 'adf_p_value': 1.0, 'critical_values': {}}
    
    @staticmethod
    def perform_autocorrelation_test_optimized(data: pd.Series, lags: int = 10) -> Dict[str, float]:
        """ìµœì í™”ëœ ìê¸°ìƒê´€ ê²€ì •"""
        if len(data) < lags + 10:
            return {'ljung_box_stat': 0.0, 'ljung_box_p_value': 1.0}
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒ˜í”Œë§
        if len(data) > 10000:
            sample_indices = np.linspace(0, len(data)-1, 10000, dtype=int)
            sample_data = data.iloc[sample_indices]
        else:
            sample_data = data
        
        try:
            # Ljung-Box ê²€ì •
            ljung_box_result = acorr_ljungbox(sample_data.dropna(), lags=lags, return_df=True)
            
            # ìµœì¢… lagì˜ ê²°ê³¼ ì‚¬ìš©
            final_lag_result = ljung_box_result.iloc[-1]
            
            return {
                'ljung_box_stat': final_lag_result['lb_stat'],
                'ljung_box_p_value': final_lag_result['lb_pvalue']
            }
            
        except Exception as e:
            logger.error(f"ìê¸°ìƒê´€ ê²€ì • ì˜¤ë¥˜: {e}")
            return {'ljung_box_stat': 0.0, 'ljung_box_p_value': 1.0}

class ParallelStatisticalAnalyzer:
    """ë³‘ë ¬ í†µê³„ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or min(mp.cpu_count(), 8)
        self.logger = logging.getLogger(__name__)
    
    def analyze_multiple_series_parallel(self, data_dict: Dict[str, pd.Series]) -> Dict[str, StatisticalResults]:
        """ì—¬ëŸ¬ ì‹œê³„ì—´ ë³‘ë ¬ ë¶„ì„"""
        def analyze_single_series(name_data_tuple):
            name, data = name_data_tuple
            analyzer = OptimizedStatisticalAnalyzer()
            results = analyzer.analyze_single_series(data)
            return name, results
        
        results = {}
        
        try:
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_name = {
                    executor.submit(analyze_single_series, item): item[0] 
                    for item in data_dict.items()
                }
                
                for future in future_to_name:
                    try:
                        name, result = future.result(timeout=30)
                        results[name] = result
                    except Exception as e:
                        self.logger.error(f"ì‹œê³„ì—´ {future_to_name[future]} ë¶„ì„ ì˜¤ë¥˜: {e}")
                        
        except Exception as e:
            self.logger.error(f"ë³‘ë ¬ ë¶„ì„ ì˜¤ë¥˜: {e}")
        
        return results

class OptimizedStatisticalAnalyzer:
    """ìµœì í™”ëœ ì¢…í•© í†µê³„ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, enable_parallel: bool = True):
        self.enable_parallel = enable_parallel
        self.logger = logging.getLogger(__name__)
        
        if enable_parallel:
            self.parallel_analyzer = ParallelStatisticalAnalyzer()
        else:
            self.parallel_analyzer = None
    
    def analyze_single_series(self, data: pd.Series) -> StatisticalResults:
        """ë‹¨ì¼ ì‹œê³„ì—´ í†µê³„ ë¶„ì„"""
        try:
            # ê¸°ë³¸ ê²€ì¦
            if data.empty:
                return StatisticalResults()
            
            # NaN ì œê±°
            clean_data = data.dropna()
            if len(clean_data) == 0:
                return StatisticalResults()
            
            data_array = clean_data.values.astype(np.float64)
            
            # ê¸°ë³¸ í†µê³„
            basic_stats = OptimizedBasicStatistics.calculate_descriptive_stats_vectorized(data_array)
            
            # ì •ê·œì„± ê²€ì •
            normality_tests = OptimizedNormalityTests.perform_normality_tests_optimized(data_array)
            
            # VaR/CVaR ê³„ì‚°
            var_cvar_results = OptimizedRiskAnalysis.calculate_var_cvar_optimized(data_array)
            
            # ì‹œê³„ì—´ ë¶„ì„
            stationarity_test = OptimizedTimeSeriesAnalysis.perform_stationarity_test_optimized(clean_data)
            autocorr_test = OptimizedTimeSeriesAnalysis.perform_autocorrelation_test_optimized(clean_data)
            
            # ê²°ê³¼ êµ¬ì„±
            results = StatisticalResults(
                # ê¸°ë³¸ í†µê³„
                count=basic_stats['count'],
                mean=basic_stats['mean'],
                std=basic_stats['std'],
                min_val=basic_stats['min'],
                max_val=basic_stats['max'],
                skewness=basic_stats['skewness'],
                kurtosis=basic_stats['kurtosis'],
                
                # ì •ê·œì„± ê²€ì •
                shapiro_wilk=normality_tests['shapiro_wilk'],
                jarque_bera=normality_tests['jarque_bera'],
                kolmogorov_smirnov=normality_tests['kolmogorov_smirnov'],
                
                # VaR/CVaR
                var_95=var_cvar_results.get('var_95', (0.0, 0.0))[0],
                var_99=var_cvar_results.get('var_99', (0.0, 0.0))[0],
                cvar_95=var_cvar_results.get('var_95', (0.0, 0.0))[1],
                cvar_99=var_cvar_results.get('var_99', (0.0, 0.0))[1],
                
                # ì‹œê³„ì—´ ë¶„ì„
                adf_test=stationarity_test,
                ljung_box=autocorr_test
            )
            
            return results
            
        except Exception as e:
            self.logger.error(f"í†µê³„ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return StatisticalResults()
    
    def analyze_comprehensive_statistics(self, trades_df: pd.DataFrame, 
                                       account_df: pd.DataFrame) -> Dict[str, StatisticalResults]:
        """ìµœì í™”ëœ ì¢…í•© í†µê³„ ë¶„ì„"""
        try:
            results = {}
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            if len(trades_df) > 100000:
                gc.collect()
            
            # 1. ê³„ì¢Œ ë°ì´í„° ë¶„ì„
            if not account_df.empty:
                # ì¼ì¼ ìˆ˜ìµë¥  ë¶„ì„
                if 'daily_return_pct' in account_df.columns:
                    daily_returns = account_df['daily_return_pct'].dropna()
                    if len(daily_returns) > 0:
                        results['DAILY_RETURNS'] = self.analyze_single_series(daily_returns)
                
                # ì”ê³  ë¶„ì„
                if 'balance' in account_df.columns:
                    balance = account_df['balance'].dropna()
                    if len(balance) > 0:
                        results['BALANCE'] = self.analyze_single_series(balance)
                
                # ë‚™í­ ë¶„ì„
                if 'drawdown_pct' in account_df.columns:
                    drawdown = account_df['drawdown_pct'].dropna()
                    if len(drawdown) > 0:
                        results['DRAWDOWN'] = self.analyze_single_series(drawdown)
            
            # 2. ê±°ë˜ ë°ì´í„° ë¶„ì„
            if not trades_df.empty:
                # P&L ë¶„ì„
                sell_trades = trades_df[trades_df['side'] == 'SELL']
                if not sell_trades.empty and 'pnl' in sell_trades.columns:
                    pnl = sell_trades['pnl'].dropna()
                    if len(pnl) > 0:
                        results['PNL'] = self.analyze_single_series(pnl)
                
                # ë³´ìœ  ê¸°ê°„ ë¶„ì„
                if 'holding_period' in sell_trades.columns:
                    holding_period = sell_trades['holding_period'].dropna()
                    if len(holding_period) > 0:
                        results['HOLDING_PERIOD'] = self.analyze_single_series(holding_period)
            
            # 3. ìƒê´€ê´€ê³„ ë¶„ì„
            correlation_data = {}
            if 'DAILY_RETURNS' in results and 'PNL' in results:
                # ì¼ì¼ ìˆ˜ìµë¥ ê³¼ P&Lì˜ ìƒê´€ê´€ê³„
                daily_returns = account_df['daily_return_pct'].dropna()
                pnl_data = sell_trades['pnl'].dropna()
                
                # ê¸¸ì´ ë§ì¶”ê¸°
                min_length = min(len(daily_returns), len(pnl_data))
                if min_length > 10:
                    correlation_data['daily_returns'] = daily_returns.values[-min_length:]
                    correlation_data['pnl'] = pnl_data.values[-min_length:]
            
            if correlation_data:
                corr_results = OptimizedCorrelationAnalysis.calculate_correlations_optimized(correlation_data)
                # ìƒê´€ê´€ê³„ ê²°ê³¼ë¥¼ StatisticalResultsì— ì¶”ê°€í•˜ëŠ” ë¡œì§
                for key in results:
                    if key in ['DAILY_RETURNS', 'PNL']:
                        results[key].pearson_corr = corr_results.get('pearson', {})
                        results[key].spearman_corr = corr_results.get('spearman', {})
            
            self.logger.info("ìµœì í™”ëœ ì¢…í•© í†µê³„ ë¶„ì„ ì™„ë£Œ")
            return results
            
        except Exception as e:
            self.logger.error(f"ì¢…í•© í†µê³„ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {}
    
    def generate_statistical_report_optimized(self, results: Dict[str, StatisticalResults]) -> str:
        """ìµœì í™”ëœ í†µê³„ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not results:
            return "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        report = "=== ìµœì í™”ëœ ìë™ë§¤ë§¤ í†µê³„ ë¶„ì„ ë¦¬í¬íŠ¸ ===\n\n"
        
        for series_name, stats in results.items():
            report += f"ğŸ“Š {series_name} ë¶„ì„\n"
            report += f"- ê´€ì¸¡ì¹˜ ìˆ˜: {stats.count:,}ê°œ\n"
            report += f"- í‰ê· : {stats.mean:.4f}\n"
            report += f"- í‘œì¤€í¸ì°¨: {stats.std:.4f}\n"
            report += f"- ìµœì†Œê°’: {stats.min_val:.4f}\n"
            report += f"- ìµœëŒ€ê°’: {stats.max_val:.4f}\n"
            report += f"- ì™œë„: {stats.skewness:.4f}\n"
            report += f"- ì²¨ë„: {stats.kurtosis:.4f}\n\n"
            
            # ì •ê·œì„± ê²€ì • ê²°ê³¼
            if stats.shapiro_wilk:
                report += "ğŸ” ì •ê·œì„± ê²€ì • ê²°ê³¼\n"
                report += f"Shapiro-Wilk: {'ì •ê·œë¶„í¬' if stats.shapiro_wilk['p_value'] > 0.05 else 'ì •ê·œë¶„í¬ ì•„ë‹˜'} "
                report += f"(p-value: {stats.shapiro_wilk['p_value']:.4f})\n"
                
                report += f"Jarque-Bera: {'ì •ê·œë¶„í¬' if stats.jarque_bera['p_value'] > 0.05 else 'ì •ê·œë¶„í¬ ì•„ë‹˜'} "
                report += f"(p-value: {stats.jarque_bera['p_value']:.4f})\n"
                
                report += f"Kolmogorov-Smirnov: {'ì •ê·œë¶„í¬' if stats.kolmogorov_smirnov['p_value'] > 0.05 else 'ì •ê·œë¶„í¬ ì•„ë‹˜'} "
                report += f"(p-value: {stats.kolmogorov_smirnov['p_value']:.4f})\n\n"
            
            # VaR/CVaR
            report += "âš ï¸ ë¦¬ìŠ¤í¬ í†µê³„\n"
            report += f"VaR (95%): {stats.var_95:.4f}\n"
            report += f"VaR (99%): {stats.var_99:.4f}\n"
            report += f"CVaR (95%): {stats.cvar_95:.4f}\n"
            report += f"CVaR (99%): {stats.cvar_99:.4f}\n\n"
            
            # ì‹œê³„ì—´ ë¶„ì„
            if stats.adf_test:
                report += "ğŸ“ˆ ì‹œê³„ì—´ ë¶„ì„\n"
                stationarity = "ì •ìƒì„±" if stats.adf_test['adf_p_value'] < 0.05 else "ë¹„ì •ìƒì„±"
                report += f"ADF ê²€ì •: {stationarity} (p-value: {stats.adf_test['adf_p_value']:.4f})\n"
            
            if stats.ljung_box:
                autocorr = "ìê¸°ìƒê´€ ìˆìŒ" if stats.ljung_box['ljung_box_p_value'] < 0.05 else "ìê¸°ìƒê´€ ì—†ìŒ"
                report += f"Ljung-Box ê²€ì •: {autocorr} (p-value: {stats.ljung_box['ljung_box_p_value']:.4f})\n"
            
            report += "\n" + "="*50 + "\n\n"
        
        return report.strip()

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    from data_processor_optimized import OptimizedDataProcessor, DataConfig
    import time
    
    # ìµœì í™”ëœ ì„¤ì •
    config = DataConfig(
        db_path="data/optimized_trading.db",
        data_period_days=90
    )
    
    # ë°ì´í„° ë¡œë“œ
    processor = OptimizedDataProcessor(config)
    trades = processor.load_trade_data_optimized()
    account = processor.load_account_history_optimized()
    
    if not trades.empty and not account.empty:
        # ë°ì´í„° ì „ì²˜ë¦¬
        processed_trades = processor.preprocess_trade_data_optimized(trades)
        processed_account = processor.preprocess_account_data_optimized(account)
        
        # ìµœì í™”ëœ í†µê³„ ë¶„ì„
        start_time = time.time()
        
        analyzer = OptimizedStatisticalAnalyzer(enable_parallel=True)
        results = analyzer.analyze_comprehensive_statistics(processed_trades, processed_account)
        
        analysis_time = time.time() - start_time
        
        # ë¦¬í¬íŠ¸ ì¶œë ¥
        report = analyzer.generate_statistical_report_optimized(results)
        print(report)
        
        print(f"\n=== ì„±ëŠ¥ ì •ë³´ ===")
        print(f"í†µê³„ ë¶„ì„ ì‹œê°„: {analysis_time:.3f}ì´ˆ")
        print(f"ë¶„ì„ëœ ì‹œê³„ì—´ ìˆ˜: {len(results)}")
        print(f"ê±°ë˜ ë°ì´í„° í¬ê¸°: {len(processed_trades):,}ê±´")
        print(f"ê³„ì¢Œ ë°ì´í„° í¬ê¸°: {len(processed_account):,}ê±´")
        
        # ìƒì„¸ ê²°ê³¼ ì¶œë ¥
        for name, stats in results.items():
            print(f"\n{name}:")
            print(f"  - í‰ê· : {stats.mean:.4f}")
            print(f"  - í‘œì¤€í¸ì°¨: {stats.std:.4f}")
            print(f"  - ì™œë„: {stats.skewness:.4f}")
            print(f"  - ì²¨ë„: {stats.kurtosis:.4f}")
    else:
        print("í†µê³„ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")



