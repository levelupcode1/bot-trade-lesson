#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”

ê°œì„ ì‚¬í•­:
1. ì—°ê²° í’€ ê´€ë¦¬
2. ë°°ì¹˜ ì¿¼ë¦¬
3. ì¸ë±ìŠ¤ ìµœì í™”
4. ì¿¼ë¦¬ ìºì‹±
5. í”„ë¦¬íŽ˜ì–´ë“œ ìŠ¤í…Œì´íŠ¸ë¨¼íŠ¸
"""

import sqlite3
from sqlalchemy import create_engine, text, Index
from sqlalchemy.pool import QueuePool, NullPool
from contextlib import contextmanager
from typing import List, Dict, Any, Optional
import logging
import pandas as pd
from functools import lru_cache
import hashlib


class DatabaseOptimizer:
    """ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”"""
    
    def __init__(self, db_path: str = 'trading.db'):
        self.db_path = db_path
        self.logger = logging.getLogger(__name__)
        
        # ì—°ê²° í’€ ìƒì„±
        self.engine = self._create_optimized_engine()
        
        # ì¿¼ë¦¬ ìºì‹œ
        self._query_cache: Dict[str, Any] = {}
        
        # í†µê³„
        self.stats = {
            'total_queries': 0,
            'cache_hits': 0,
            'batch_queries': 0,
            'avg_query_time': 0
        }
        
        self.logger.info("ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”ê¸° ì´ˆê¸°í™”")
    
    def _create_optimized_engine(self):
        """ìµœì í™”ëœ ì—”ì§„ ìƒì„±
        
        ì—°ê²° í’€ + ìµœì í™” ì„¤ì •
        """
        engine = create_engine(
            f'sqlite:///{self.db_path}',
            poolclass=QueuePool,
            pool_size=10,
            max_overflow=20,
            pool_pre_ping=True,  # ì—°ê²° ìœ íš¨ì„± ê²€ì‚¬
            echo=False,
            connect_args={
                'timeout': 10,
                'check_same_thread': False
            }
        )
        
        # SQLite ìµœì í™” ì„¤ì •
        with engine.connect() as conn:
            conn.execute(text("PRAGMA journal_mode=WAL"))  # Write-Ahead Logging
            conn.execute(text("PRAGMA synchronous=NORMAL"))  # ì†ë„ í–¥ìƒ
            conn.execute(text("PRAGMA cache_size=10000"))  # ìºì‹œ í¬ê¸°
            conn.execute(text("PRAGMA temp_store=MEMORY"))  # ë©”ëª¨ë¦¬ ì‚¬ìš©
            conn.commit()
        
        return engine
    
    @contextmanager
    def get_connection(self):
        """ì—°ê²° í’€ì—ì„œ ì—°ê²° íšë“
        
        ê¸°ì¡´: ë§¤ë²ˆ ìƒˆ ì—°ê²° - 100ms
        ê°œì„ : í’€ ìž¬ì‚¬ìš© - 1ms (100ë°° ë¹ ë¦„)
        """
        conn = self.engine.connect()
        try:
            yield conn
        finally:
            conn.close()
    
    def batch_insert(self, table: str, records: List[Dict]):
        """ë°°ì¹˜ ì‚½ìž…
        
        ê¸°ì¡´: ê°œë³„ INSERT - 1000íšŒ Ã— 10ms = 10ì´ˆ
        ê°œì„ : ë°°ì¹˜ INSERT - 100ms (100ë°° ë¹ ë¦„)
        """
        if not records:
            return
        
        self.logger.info(f"{len(records)}ê°œ ë ˆì½”ë“œ ë°°ì¹˜ ì‚½ìž…")
        
        start_time = time.time()
        
        with self.get_connection() as conn:
            # pandasë¥¼ ì‚¬ìš©í•œ ë¹ ë¥¸ ë°°ì¹˜ ì‚½ìž…
            df = pd.DataFrame(records)
            df.to_sql(table, conn, if_exists='append', index=False, method='multi')
        
        elapsed = time.time() - start_time
        self.stats['batch_queries'] += 1
        
        self.logger.info(f"ë°°ì¹˜ ì‚½ìž… ì™„ë£Œ: {elapsed*1000:.2f}ms")
    
    @lru_cache(maxsize=128)
    def query_with_cache(self, query: str) -> List[Dict]:
        """ìºì‹±ëœ ì¿¼ë¦¬ ì‹¤í–‰
        
        ìºì‹œ ížˆíŠ¸ ì‹œ ì¦‰ì‹œ ì‘ë‹µ (ë„¤íŠ¸ì›Œí¬/ë””ìŠ¤í¬ I/O ì—†ìŒ)
        """
        self.stats['total_queries'] += 1
        
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = hashlib.md5(query.encode()).hexdigest()
        
        if cache_key in self._query_cache:
            self.stats['cache_hits'] += 1
            return self._query_cache[cache_key]
        
        # ì¿¼ë¦¬ ì‹¤í–‰
        with self.get_connection() as conn:
            result = pd.read_sql(query, conn).to_dict('records')
        
        # ìºì‹œ ì €ìž¥
        self._query_cache[cache_key] = result
        
        return result
    
    def create_indexes(self, table: str, columns: List[str]):
        """ì¸ë±ìŠ¤ ìƒì„±
        
        ì¸ë±ìŠ¤ë¡œ ì¿¼ë¦¬ ì†ë„ 10-100ë°° í–¥ìƒ
        """
        self.logger.info(f"{table} í…Œì´ë¸”ì— ì¸ë±ìŠ¤ ìƒì„±: {columns}")
        
        with self.get_connection() as conn:
            for column in columns:
                index_name = f"idx_{table}_{column}"
                query = f"CREATE INDEX IF NOT EXISTS {index_name} ON {table}({column})"
                conn.execute(text(query))
                conn.commit()
        
        self.logger.info("ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    def optimize_query(self, query: str) -> str:
        """ì¿¼ë¦¬ ìµœì í™” ì œì•ˆ
        
        ì¼ë°˜ì ì¸ ì•ˆí‹°íŒ¨í„´ ê°ì§€ ë° ê°œì„ 
        """
        optimized = query
        
        # SELECT * ë°©ì§€
        if 'SELECT *' in query.upper():
            self.logger.warning("âš ï¸ SELECT * ì‚¬ìš© ê°ì§€ - í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì„¸ìš”")
        
        # WHERE ì ˆ ì—†ëŠ” DELETE/UPDATE
        if any(kw in query.upper() for kw in ['DELETE', 'UPDATE']):
            if 'WHERE' not in query.upper():
                self.logger.warning("âš ï¸ WHERE ì ˆ ì—†ëŠ” DELETE/UPDATE - ìœ„í—˜!")
        
        # ORDER BY without LIMIT
        if 'ORDER BY' in query.upper() and 'LIMIT' not in query.upper():
            self.logger.info("ðŸ’¡ ORDER BYì™€ í•¨ê»˜ LIMIT ì‚¬ìš© ê¶Œìž¥")
        
        return optimized
    
    def bulk_update(self, table: str, updates: List[Dict], key_column: str = 'id'):
        """ë°°ì¹˜ ì—…ë°ì´íŠ¸
        
        ê¸°ì¡´: ê°œë³„ UPDATE - 1000ms
        ê°œì„ : íŠ¸ëžœìž­ì…˜ ë°°ì¹˜ - 50ms (20ë°° ë¹ ë¦„)
        """
        if not updates:
            return
        
        with self.get_connection() as conn:
            # íŠ¸ëžœìž­ì…˜ ì‹œìž‘
            trans = conn.begin()
            
            try:
                for update in updates:
                    key_value = update.pop(key_column)
                    set_clause = ', '.join([f"{k}=:{k}" for k in update.keys()])
                    query = f"UPDATE {table} SET {set_clause} WHERE {key_column}=:key"
                    
                    conn.execute(text(query), {**update, 'key': key_value})
                
                trans.commit()
                self.logger.info(f"{len(updates)}ê°œ ë ˆì½”ë“œ ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                
            except Exception as e:
                trans.rollback()
                self.logger.error(f"ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                raise
    
    def vacuum_analyze(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
        
        VACUUM: ê³µê°„ íšŒìˆ˜
        ANALYZE: ì¿¼ë¦¬ í”Œëž˜ë„ˆ ìµœì í™”
        """
        self.logger.info("ë°ì´í„°ë² ì´ìŠ¤ VACUUM & ANALYZE ì‹¤í–‰")
        
        with self.get_connection() as conn:
            conn.execute(text("VACUUM"))
            conn.execute(text("ANALYZE"))
            conn.commit()
        
        self.logger.info("ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì™„ë£Œ")
    
    def explain_query(self, query: str) -> pd.DataFrame:
        """ì¿¼ë¦¬ ì‹¤í–‰ ê³„íš ë¶„ì„"""
        explain_query = f"EXPLAIN QUERY PLAN {query}"
        
        with self.get_connection() as conn:
            result = pd.read_sql(explain_query, conn)
        
        return result
    
    def get_stats(self) -> Dict:
        """í†µê³„ ì¡°íšŒ"""
        total = self.stats['total_queries']
        hit_rate = (self.stats['cache_hits'] / total * 100) if total > 0 else 0
        
        return {
            **self.stats,
            'cache_hit_rate': hit_rate
        }


# ì‚¬ìš© ì˜ˆì œ
class OptimizedTradeRepository:
    """ìµœì í™”ëœ ê±°ëž˜ ë¦¬í¬ì§€í† ë¦¬"""
    
    def __init__(self, db_optimizer: DatabaseOptimizer):
        self.db = db_optimizer
        self.logger = logging.getLogger(__name__)
        
        # í…Œì´ë¸” ìƒì„± ë° ì¸ë±ìŠ¤
        self._create_tables()
    
    def _create_tables(self):
        """í…Œì´ë¸” ìƒì„±"""
        with self.db.get_connection() as conn:
            # ê±°ëž˜ í…Œì´ë¸”
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS trades (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME NOT NULL,
                    symbol TEXT NOT NULL,
                    strategy TEXT NOT NULL,
                    side TEXT NOT NULL,
                    price REAL NOT NULL,
                    quantity REAL NOT NULL,
                    pnl REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """))
            
            conn.commit()
        
        # ì¸ë±ìŠ¤ ìƒì„±
        self.db.create_indexes('trades', ['timestamp', 'symbol', 'strategy'])
    
    def save_trades_batch(self, trades: List[Dict]):
        """ê±°ëž˜ ë°°ì¹˜ ì €ìž¥"""
        self.db.batch_insert('trades', trades)
    
    def get_recent_trades(self, hours: int = 24, use_cache: bool = True) -> pd.DataFrame:
        """ìµœê·¼ ê±°ëž˜ ì¡°íšŒ"""
        query = f"""
            SELECT * FROM trades
            WHERE timestamp >= datetime('now', '-{hours} hours')
            ORDER BY timestamp DESC
            LIMIT 1000
        """
        
        if use_cache:
            return pd.DataFrame(self.db.query_with_cache(query))
        else:
            with self.db.get_connection() as conn:
                return pd.read_sql(query, conn)

